# -*- coding: utf-8 -*-
"""Optimization of search engine feedback through query expansion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qXYmQU9aOcYg2vvnWUsJvLOi_Nteu3uU
"""
import nltk

nltk.download("wordnet")
nltk.download("stopwords")
nltk.download("punkt")
nltk.download("averaged_perceptron_tagger")

import asynctask
from asynctask import *
import messagelogger
import prd_retrieval
import wordnet_query_expansion
from bs4 import BeautifulSoup
import requests
import json
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import sys
import wikipediaapi
from nltk.corpus import wordnet
import math
import custom_search
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import time 


logger = messagelogger.Logger("query_expansion_logs")

def clear_html_tags(html):
    return BeautifulSoup(html,'html.parser').text.strip()

# Returns an item from a list of lists based on the best borda rank
def bordaCountSelection(lists):
    if len(lists) < 1: return False
    # Make sure all the lists have the same number of elements
    size = len(lists[0])
    for i in lists:
        if len(i) != size: return False

    # rank in format {<item>: <rank>}
    ranks = {}
    for results in lists:
        pos = 1
        for item in results:
            new_rank = size - pos
            pos += 1
            if item in ranks: ranks[item] += new_rank
            else: ranks[item] = new_rank

    sorted_ranks = sorted(ranks.items(), key=lambda x:x[1], reverse=True)
    #print(sorted_ranks)
    # return the items with the best rank
    best_ranked_items = []
    best_rank = sorted_ranks[0][1]

    #get more items (incase of a tie)
    [best_ranked_items.append(x[0]) for x in sorted_ranks if x[1] == best_rank]
    #the caller handles multiple best items
    return best_ranked_items
    
def bordaCountSelectionTest():
    # contains test for the bordaCoutSelection function
    results = [[], [], []]
    results[0].append('president of the senate of nigeria')
    results[0].append('president of nigeria')
    results[0].append('vice president of nigeria')
    results[0].append('list of head of states of nigeria')
    results[0].append('nnamdi azikwe')

    results[1].append('president of nigeria')
    results[1].append('president of the senate of nigeria')
    results[1].append('vice president of nigeria')
    results[1].append('list of head of states of nigeria')
    results[1].append('chief of staff to the president')

    results[2].append('george washington')
    results[2].append('the first president')
    results[2].append('chester a. arthur')
    results[2].append('president of the philipines')
    results[2].append('list of presidents of the united states')

    result = bordaCountSelection(results)
    expected = ''
    print(result)
    if len(result) != 2: raise Exception('Test failed!')

def tokenize(word):
    tokens = word_tokenize(word)
    badwords = stopwords.words('english')
    tokens = [x for x in tokens if x not in badwords]
    return tokens

class QueryTask(Task):
    def __init__(self, task_id, query, max_attempts = 3):
        Task.__init__(self, task_id)
        self.query = query
        # number of times to try if attempts request fails before giving up
        self.__max_attempts = max_attempts
        # self.request = "https://en.wikipedia.org/w/api.php?action=query&generator=search&gsrsearch=" + query + "&prop=&format=json&gsrlimit=5"
        self.request = "https://en.wikipedia.org/w/api.php?action=query&list=search&inprop=url&utf8=&format=json&srlimit=5&srsearch=" + query

    def execute(self):
        wiki_titles = []
        for i in range(self.__max_attempts):
            # get wikipedia articles from wikipedia
            try:
                logger.log('attempting connection for: {}'.format(self.request))
                response = requests.get(self.request)
                if response.status_code == 200:
                    json_response = json.loads(response.content)

                    # do some more processing to obain results
                    results = json_response['query']['search']
                    for result in results:
                        wiki_titles.append(result['title'])

                    if len(wiki_titles) > 0: self.setResult(wiki_titles)
                    break
                else:
                    print('Invalid response')
            except ConnectionError as error:
                print('Connection Error: Unable to connect to Wikipedia Servers')
            except Exception as error:
                print('An error occurred!')

               
def getBestArticle(queries):
    ''' Returns a wikipedia article that is most relevant to the query based on border count rank method, 
    throws an exception if unable to get article. '''
    
    if len(queries) > 0:
        if len(queries) > 1:
            #individual words will not be used
            queries = queries[:len(queries) - 1]
        new_queries = []
        for x in queries: new_queries += x
        logger.log('list of phrases to obtain best article: {}'.format(new_queries))

        taskManager = TaskManager()
        id = 0
        for q in new_queries:
            taskManager.addAsyncTask(QueryTask(id, q))
            id += 1

        taskManager.startAsyncTasks()
        logger.log('waiting for search results...')
        taskManager.joinAllTasks()

        results = [x.result for x in taskManager.getTasks() if x.result != None]
        #print(results)

        # can be used to check if a result was actually gotten
        is_retrieved = False

        if len(results) > 0:
            logger.log('Top articles: {}'.format(results))
            result = bordaCountSelection(results)
            #print('best titles', result)

            query_tokens = tokenize(query.lower())
            ranks = {}
            if result != False:
                for i in result:
                    a = set(tokenize(i.lower()))
                    rank = len(set(query_tokens).intersection(a))/len(a)
                    ranks[i] = rank
                ranks = sorted(ranks.items(), key=lambda x:x[1], reverse=True)
                #print('best item:', ranks[0][0])
                return ranks[0][0]
        raise Exception('Unable to get the best relevant article')
    else: raise Exception('Invalid query argument, query list cannot be empty')

def getBestArticlesFromGoogle(original_query):
    # get the first result from the google result
    result = custom_search.getGoogleResults(original_query)
    items = result['items']
    best_items = [items[x]['title'].replace(" - Wikipedia", "") for x in range(len(items))]
    return {'usedQuery': result['searchQuery'], 'best_articles': best_items}
    
def makeRequest(session, url, timeout=10):
    response = session.get(url, timeout=10)
    if response.status_code == 200: return response.content
    else: raise Exception('request failed to complete')

def makeRequestsAsync(session, tasksInfo):
    tasks = []
    for index in range(len(tasksInfo['titles'])):
        new_task = AsyncTaskHandler(lambda: makeRequest(session, tasksInfo['urls'][index]), taskInfo=tasksInfo['titles'][index])
        new_task.start()
        tasks.append(new_task)
    return tasks

def getWikiPageContent(title, pagelinks=True, session=None):
    ''' Returns the content of a page and optionally the links contained within the page '''

    url = "https://en.wikipedia.org/w/api.php?action=parse&format=json&page=" + title
    if pagelinks: url += "&prop=links|wikitext|"
    else: url += "&prop=wikitext|"

    # make use of a session if its is available
    if session is not None: response = session.get(url, timeout=10)
    else: response = requests.get(url, timeout=10)

    if response.status_code == 200:
        content = json.loads(response.content)['parse']
        text = prd_retrieval.clear_html_tags(content['wikitext']['*'])
        if pagelinks:
            links = content['links']
            m_links = [dic['*'] for dic in links]
            return {'links': m_links, 'text': text}
        return {'text': text}
    raise Exception('failed to get wikipage content')

def getBacklinks(page):
    ''' Returns all the backlinks to specified page in the main namespace. '''
    main_url = "https://en.wikipedia.org/w/api.php?action=query&format=json&list=backlinks&bltitle={}&blnamespace=0&blredirect=false&bllimit=500".format(page)
    url = main_url
    m_next = True
    all_backlinks = []
    count = 0
    while m_next:
        m_next = False
        logger.log('making request ' + str(count) + ' to: ' + url)
        response = requests.get(url)
        if response.status_code == 200:
            result = json.loads(response.content)
            if 'continue' in result:
                m_next = True
                url = "{}&blcontinue={}&continue={}".format(main_url, result['continue']['blcontinue'], result['continue']['continue'])
            backlinks = result['query']['backlinks']
            links = [x['title'] for x in backlinks]
            all_backlinks += links
            #logger.log('found: ' + str(len(links)) + ' links. Total: ' + str(len(all_backlinks)) + ' links')
        else:
            logger.log('an error occurred and could not continue...')
            break
        count += 1
    return all_backlinks

def get_doc_term_matrix(docs, doc_titles):
    # create the document term matrix
    vectorizer = TfidfVectorizer(stop_words="english")
    sparse_matrix = vectorizer.fit_transform(docs)

    #convert sparse matrix to pandas dataframe
    doc_term_matrix = sparse_matrix.todense()
    df = pd.DataFrame(doc_term_matrix, columns=vectorizer.get_feature_names_out(), index=[doc_titles])

    return sparse_matrix

def get_cosine_similarity(sparse_matrix, doc_term_matrix):
    return cosine_similarity(sparse_matrix, doc_term_matrix)

def get_candidate_expansion_terms(query):
    if len(query) > 1:        
        s_time = time.time()

        logger.start_log_to_file()
        logger.log('Processing, getting expansion terms...')        
        logger.log('original query: ' + query)
        
        cand_expansion_terms = None
        try:
            # <<1>> obtain the best ranked article that relates to the query terms using border count rank
            #title = getBestArticle(queries)            
            query = query.lower()            
            s_result = getBestArticlesFromGoogle(query)

            logger.log("google search query used: " + s_result['usedQuery'])

            main_title = s_result['best_articles'][0]
            logger.log('Best article from Google Wikipedia Search-->> {}'.format(main_title))
            logger.log('getting best article content...')                     
            main_page_content = getWikiPageContent(main_title)
            out_links_a = main_page_content['links']

            second_main_title = None
            # get the next item in the result
            for i in s_result['best_articles']:
                if i in out_links_a:
                    second_main_title = i
                    break
            if second_main_title == None: 
                logger.log("No other result is linked to the main article")
                second_main_title = s_result['best_articles'][1]
                logger.log('Second best article from Google Wikipedia Search-->> {}'.format(second_main_title))
            else:
                logger.log('Second best article from Google Wikipedia Search linked to the best article-->> {}'.format(second_main_title))       
            
            logger.log('Done in {} seconds'.format(time.time() - s_time))
            print()            
                        
            logger.log('getting second best article content...') 
            second_main_page_content = getWikiPageContent(second_main_title)

            # <<3>> obtain the links to the best article
            logger.log('getting in_links of best articles...')
            # wiki = wikipediaapi.Wikipedia('en')
            # in_links = wiki.page(main_title).backlinks
            in_links_a = getBacklinks(main_title)
            in_links_b = getBacklinks(second_main_title)
            
            logger.log('done getting backlinks of best articles')
            
            out_links_b = second_main_page_content['links']

            logger.log('{} contains {} in-links and {} out-links'.format(main_title, len(in_links_a), len(out_links_a)))
            logger.log('{} contains {} in-links and {} out-links'.format(second_main_title, len(in_links_b), len(out_links_b)))
            
            # titles are now referred to as links
            # filteration of non main-namespace links
            #in_links = [x for x in in_links if ':' not in x]
            in_links_a = set(in_links_a)
            out_links_a = set(out_links_a)

            common_links_a = out_links_a.intersection(in_links_a)
            logger.log('{} has {} semantically related docs'.format(main_title, len(common_links_a)))

            in_links_b = set(in_links_b)
            out_links_b = set(out_links_b)

            common_links_b = out_links_b.intersection(in_links_b)
            logger.log('{} has {} semantically related docs'.format(second_main_title, len(common_links_b)))

            common_links = common_links_a.union(common_links_b)

            logger.log('{} documents are semantically related to either {} or {} or both'.format(len(common_links), main_title, second_main_title))
            
            semantically_related_docs = {}

            # prepare a dictionary of semantically related documents that will be used hold all its data
            for title in common_links:
                semantically_related_docs[title] = {'doc_content': None, 'tf': 0, 'idf': None, 'tokenized_content': None}

            logger.log("Done in {} seconds".format(time.time() - s_time))
            print()

            # <<4>> obtain the content of the semantically related articles
            m_session = requests.Session()
            logger.log('getting semantically related page contents')
            tasks = []
            for title in semantically_related_docs:
                new_task = AsyncTaskHandler(lambda: getWikiPageContent(title, pagelinks=False, session=m_session), taskInfo=title)
                new_task.start()
                tasks.append(new_task)
            logger.log('started {} tasks'.format(len(tasks)))

            count = 1
            for task in tasks:
                try:
                    task.join()
                    title = task.getTaskInfo()
                    semantically_related_docs[title]['doc_content'] = task.getResult()['text']
                    #logger.logSingleLine('{}/{} document content retrieved...'.format(count, len(tasks)))
                    count += 1
                except Exception as error:
                    print()
                    logger.log('Error: something went wrong while retrieving page content, aborting...{}'.format(error))
                    m_session.close()
                    break
            logger.log("Got {}/{} semantically related docs page content".format(count, len(tasks)))
            # incase session had been closed in previous operation
            m_session = requests.Session()

            logger.log('Computing the cosine similarity of all the sematically related documents')
            # calculating measure of similarity of semantically related docs to the main article
            docs_titles = ['combined_articles', main_title, second_main_title] + [title for title in semantically_related_docs if semantically_related_docs[title]['doc_content'] is not None]

            combined_docs = main_page_content['text'] + ' ' + second_main_page_content['text']
            docs_contents = [combined_docs, main_page_content['text'], second_main_page_content['text']] + [semantically_related_docs[title]['doc_content'] for title in semantically_related_docs if semantically_related_docs[title]['doc_content'] is not None]
            
            main_article_index = 0
            doc_term_matrix = get_doc_term_matrix(docs_contents, docs_titles)
            cos_similarity = get_cosine_similarity(doc_term_matrix[main_article_index], doc_term_matrix)            
            cos_similarity = cos_similarity[main_article_index]

            #print(cos_similarity)
            logger.log("Printing Cosine similarity of documents: ")
            
            dc = {}
            for i in range(len(docs_titles)):
                dc[docs_titles[i]] = cos_similarity[i]

            # sort the document base on cosine similarity
            dc = dict(sorted(dc.items(), key=lambda x: x[1], reverse=True))

            for title in dc:
                logger.log('{}     {}'.format(('title: ' + title).ljust(70), ('cosine similarity: ' + str(dc[title])).ljust(50)))
            logger.log("Printing candidate expansion terms: ")            
            
            cand_expansion_terms = list(dc.keys())[1:]
            #logger.log(cand_expansion_terms)
            
            logger.log('all done in {} seconds'.format(time.time() - s_time))
        
            logger.log("Printing All wikipedia candidate expansion terms:")
            for i in cand_expansion_terms:
                logger.log(i)
            logger.end_logging_session()
            return {'raw_expansion_terms': cand_expansion_terms, 'query': s_result['usedQuery']}
        except Exception as error:
            logger.log("Error: Task aborted, an error has occurred: {}".format(error))
            return None
    else:
        return None

if __name__ == '__main__':
    query = input('Enter query to expand: ')
    result = get_candidate_expansion_terms(query)
    if result is not None:
        cand_expansion_terms = result['raw_expansion_terms']
        ts = TermSelector()
        cats = ts.refine_terms(cand_expansion_terms, result['query'])

        ex_terms = "";
        for i in cats[1][:5]:
            ex_terms +=i + ' '
        ex_terms.strip()
    
        expanded_query = result['query'] + ' ' + ex_terms
        print('Original query: ' + result['query'])
        print('Expanded query: ' + expanded_query)
    else:
        print('error')